{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d282b2d",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d16729c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlalchemy as sqla\n",
    "import pickle\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pymysql\n",
    "from sshtunnel import SSHTunnelForwarder\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Load pre-trained word embeddings (e.g., spaCy's medium English model)\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "\n",
    "from shapely.geometry import MultiPolygon, Polygon\n",
    "from shapely.ops import nearest_points\n",
    "import geopandas as gpd\n",
    "from shapely import wkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf165be8",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data_models/Recommendation_model/venue_static.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_venue_static \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39m../data_models/Recommendation_model/venue_static.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      2\u001b[0m df_venue_timings \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39m../data_models/Recommendation_model/venue_timings.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m df_venue_merged \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39m../data_models/Recommendation_model/venue_merged.csv\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32md:\\Python38\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\Python38\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\Python38\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32md:\\Python38\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32md:\\Python38\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32md:\\Python38\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1736\u001b[0m     f,\n\u001b[0;32m   1737\u001b[0m     mode,\n\u001b[0;32m   1738\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1739\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1740\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1741\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1742\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1743\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1744\u001b[0m )\n\u001b[0;32m   1745\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32md:\\Python38\\lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    859\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    860\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    861\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data_models/Recommendation_model/venue_static.csv'"
     ]
    }
   ],
   "source": [
    "df_venue_static = pd.read_csv('../data_models/Recommendation_model/venue_static.csv')\n",
    "df_venue_timings = pd.read_csv('../data_models/Recommendation_model/venue_timings.csv')\n",
    "df_venue_merged = pd.read_csv('../data_models/Recommendation_model/venue_merged.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5810f8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_venue_static = df_venue_static[df_venue_static['venue_type'] != 'LIBRARY']\n",
    "df_venue_static.to_csv('../data_models/Recommendation_model/venue_static.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca397e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manhattan_zone = pd.read_csv('../data_models/Recommendation_model/manhattan_zones.csv')\n",
    "#df_manhattan_zone.head(50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e84e1ef",
   "metadata": {},
   "source": [
    "# Manipulate Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f471e79",
   "metadata": {},
   "source": [
    "## Split into Hour and Day of the week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b31d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_venue_merged['merged_time'] = pd.to_datetime(df_venue_merged['merged_time'])\n",
    "\n",
    "# Add 'day_of_week' column (Monday as 0)\n",
    "df_venue_merged['day_of_week'] = df_venue_merged['merged_time'].dt.dayofweek\n",
    "\n",
    "# Add 'hour_integer' column\n",
    "df_venue_merged['hour_integer'] = df_venue_merged['merged_time'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab6ae57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(df_venue_merged.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78812016",
   "metadata": {},
   "source": [
    "## Grouping Venue Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18f1e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "venue_mapping = {\n",
    "    'PARK': 'Park',\n",
    "    'TOURIST_DESTINATION': 'Tourist Destination',\n",
    "    'MUSEUM': 'Cultural Heritage',\n",
    "    'HISTORICAL':'Cultural Heritage',\n",
    "    'SCENIC_POINT': 'Scenic Landmarks',\n",
    "    'BRIDGE': 'Scenic Landmarks',\n",
    "    'NATURE_RESERVE': 'Nature Attractions',\n",
    "    'ZOO': 'Nature Attractions',\n",
    "    'BOTANICAL_GARDEN': 'Nature Attractions',\n",
    "    'ARTS': 'Art',\n",
    "    'DESSERT':'Art',\n",
    "    'CHURCH': 'Religious',\n",
    "    'SYNAGOGUE':'Religious',\n",
    "    'VISITOR_CENTER': 'Tourist Destination',\n",
    "    'LIBRARY':'Library',\n",
    "    'SHOPPING_CENTER': 'Shopping Center',\n",
    "    'APPAREL':'Fashion Convenience',\n",
    "    'OTHER': 'Tourist Destination',\n",
    "    'SHOPPING': 'Fashion Convenience',\n",
    "    'CONVENIENCE_STORE':'Neighborhood Market',\n",
    "    'SUPERMARKET': 'Neighborhood Market',\n",
    "    'GROCERY':'Neighborhood Market',\n",
    "    'MARKET':'Neighborhood Market',\n",
    "    'GIFTS': 'Gifts & Souvenirs',\n",
    "    'SOUVENIR_SHOP':'Gifts & Souvenirs',\n",
    "    \n",
    "}\n",
    "\n",
    "df_venue_static['venue_mod_type'] = df_venue_static['venue_type'].replace(venue_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d77b855",
   "metadata": {},
   "outputs": [],
   "source": [
    "venue_to_zone_dict = {}\n",
    "\n",
    "# Iterate through each row in the DataFrame\n",
    "for index, row in df_venue_static.iterrows():\n",
    "    venue_type = row['venue_mod_type']\n",
    "    zone = row['zone_id']\n",
    "    \n",
    "    # If the venue_type is already in the dictionary, append the zone to its list\n",
    "    if venue_type in venue_to_zone_dict:\n",
    "        venue_to_zone_dict[venue_type].append(zone)\n",
    "    # If the venue_type is not in the dictionary, create a new entry with the zone as a list\n",
    "    else:\n",
    "        venue_to_zone_dict[venue_type] = [zone]\n",
    "\n",
    "#print(venue_to_zone_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202966d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_venue_static.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd513591",
   "metadata": {},
   "source": [
    "# Clearing Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956cabca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look for duplicates\n",
    "#print('Number of duplicate (excluding first) rows in the table is: ', df_venue_static.duplicated().sum())\n",
    "\n",
    "# use \"keep=False\" to mark all duplicates as true, including the original rows that were duplicated\n",
    "#print('Number of duplicate rows (including first) in the table is:', df_venue_static[df_venue_static.duplicated(keep=False)].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51881083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look for duplicates\n",
    "#print('Number of duplicate (excluding first) rows in the table is: ', df_venue_timings.duplicated().sum())\n",
    "\n",
    "# use \"keep=False\" to mark all duplicates as true, including the original rows that were duplicated\n",
    "#print('Number of duplicate rows (including first) in the table is:', df_venue_timings[df_venue_timings.duplicated(subset=['venue_id', 'day', 'opening_time', 'closing_time'], keep='first')].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dee57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('Number of duplicate (excluding first) rows in the table is: ', df_venue_timings.drop_duplicates(subset=['venue_id', 'day', 'opening_time', 'closing_time'], inplace=True))\n",
    "#df_venue_timings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30a1268",
   "metadata": {},
   "source": [
    "# Grouping Zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6651ea3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "venue_zone_grouping = {\n",
    "    'Upper Manhattan': [128, 127, 243, 120, 244, 116, 42, 152, 41, 74, 75],\n",
    "    'Upper West Side': [166, 24, 151, 43, 238, 239, 143, 142],\n",
    "    'Upper East Side': [236,263, 262, 237, 141, 140 ],\n",
    "    'Chelsea/Greenwhich market':[246, 68, 186, 90, 100, 234, 158, 249, 113, 249],\n",
    "    'Lower Manhattan': [107, 224, 114, 211, 144, 148, 232, 231, 45, 13, 261, 209, 87, 88, 12 ],\n",
    "    'Midtown Manhattan': [50, 48, 230, 163, 161, 162, 229, 233, 164, 170, 137, 224, 107, 234]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc59c213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# venue_zone_grouping dictionary\n",
    "venue_zone_grouping = {\n",
    "    'Upper Manhattan': [128, 127, 243, 120, 244, 116, 42, 152, 41, 74, 75],\n",
    "    'Upper West Side': [166, 24, 151, 43, 238, 239, 143, 142],\n",
    "    'Upper East Side': [236, 263, 262, 237, 141, 140],\n",
    "    'Chelsea/Greenwhich market': [246, 68, 186, 90, 100, 234, 158, 249, 113, 249],\n",
    "    'Lower Manhattan': [107, 224, 114, 211, 144, 148, 232, 231, 45, 13, 261, 209, 87, 88, 12],\n",
    "    'Midtown Manhattan': [50, 48, 230, 163, 161, 162, 229, 233, 164, 170, 137, 224, 107, 234],\n",
    "}\n",
    "\n",
    "# Function to map zone numbers to zone groups\n",
    "def map_zone_group(zone_number):\n",
    "    for zone_group, zone_numbers in venue_zone_grouping.items():\n",
    "        if zone_number in zone_numbers:\n",
    "            return zone_group\n",
    "    return 'Other'  # If zone number not found in the dictionary, assign 'Other'\n",
    "\n",
    "# Create the 'zone_group' column based on the mapping\n",
    "df_venue_static['zone_group'] = df_venue_static['zone_id'].apply(map_zone_group)\n",
    "\n",
    "#print(df_venue_static)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b852085",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_venue_static.to_csv('../data_models/Recommendation_model/zone_Grouping.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7304da8b",
   "metadata": {},
   "source": [
    "# Extracting Only Attratcion Types and Ignoring Restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f2309b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_type_values = df_venue_static['venue_mod_type'].unique()\n",
    "#unique_type_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae74f663",
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_venue_types = ['Nature Attractions', 'Shopping Center', 'Tourist Destination', 'Cultural Heritage', 'Neighborhood Market', 'Fashion Convenience',  'Scenic Landmarks', 'Art', 'Religious', 'Park', 'Gifts & Souvenirs']\n",
    "\n",
    "# Filter the DataFrame to only include rows with the specific venue types\n",
    "df_venue_static_att = df_venue_static[df_venue_static['venue_mod_type'].isin(specific_venue_types)]\n",
    "\n",
    "# Now 'filtered_df' contains only rows where the \"Attraction_Type\" is in the specified list\n",
    "#print(df_venue_static_att)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5def6015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the restaurant types\n",
    "restaurant_types = [venue for venue in unique_type_values if 'RESTAURANT' in venue]\n",
    "\n",
    "# Assuming you have a dataframe named 'df_venue' which has a column 'venue_type'\n",
    "# that matches the values in your unique_type_values array, you can filter it as follows:\n",
    "df_venue_restaurant = df_venue_static[df_venue_static['venue_type'].isin(restaurant_types)]\n",
    "\n",
    "#print(df_venue_restaurant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1507fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_venue_types = df_venue_restaurant['venue_type'].unique()\n",
    "#print(unique_venue_types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bf8b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_type_values_att = df_venue_static_att['venue_mod_type'].unique()\n",
    "#unique_type_values_att"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd3a574",
   "metadata": {},
   "source": [
    "# Actual Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774716c2",
   "metadata": {},
   "source": [
    "### Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab6a459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "params = sys.argv[1:]\n",
    "substrings = params[0].split(',')\n",
    "user_zone_input = substrings\n",
    "user_zone_input = [attraction.replace('_', ' ') for attraction in user_zone_input]\n",
    "\n",
    "substrings = params[1].split(',')\n",
    "user_input_attractions = substrings\n",
    "user_input_attractions = [attraction.replace('_', ' ') for attraction in user_input_attractions]\n",
    "\n",
    "substrings = params[2].split(',')\n",
    "user_input_restaurants = substrings\n",
    "\n",
    "for i in range(len(user_input_restaurants)):\n",
    "    user_input_restaurants[i] = user_input_restaurants[i].upper()\n",
    "    \n",
    "\n",
    "\n",
    "# user_zone_input = [\"Chelsea/Greenwhich market\",\"Upper Manhattan\"]\n",
    "\n",
    "# user_input_attractions = [\n",
    "#         \"Tourist Destination\",\n",
    "#         \"Fashion Convenience\",\n",
    "#         \"Neighborhood Market\",\n",
    "#         \"Shopping Center\"\n",
    "#     ]\n",
    "\n",
    "# user_input_restaurants = ['FRENCH_RESTAURANT', 'ITALIAN_RESTAURANT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91c9663",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(user_input_attractions) < 4:\n",
    "    x = 4 - len(user_input_attractions)\n",
    "    \n",
    "    updated_list = [num for num in unique_type_values_att if num not in user_input_attractions]\n",
    "    \n",
    "    # Always include either 'Park', 'Scenic Landmark', or 'Tourist Destination' if not in user's input\n",
    "    core_attractions = ['Park', 'Scenic Landmarks', 'Tourist Destination']\n",
    "    \n",
    "    # Find out which core attractions are not in the user's input\n",
    "    missing_core_attractions = [attraction for attraction in core_attractions if attraction not in user_input_attractions]\n",
    "    \n",
    "    # Compute similarities only for missing core attractions\n",
    "    core_similarities = []\n",
    "    user_input_tag_embedding = nlp(user_input_attractions[0]).vector\n",
    "\n",
    "    for tag in missing_core_attractions:\n",
    "        tag_embedding = nlp(tag).vector\n",
    "        similarity = user_input_tag_embedding.dot(tag_embedding) / (np.linalg.norm(user_input_tag_embedding) * np.linalg.norm(tag_embedding))\n",
    "        core_similarities.append(similarity)\n",
    "\n",
    "    # Add the most similar core attraction to user's input\n",
    "    if core_similarities:\n",
    "        most_similar_core_index = np.argmax(core_similarities)\n",
    "        user_input_attractions.append(missing_core_attractions[most_similar_core_index])\n",
    "        x -= 1  # Decrement x as we've added a core attraction\n",
    "        \n",
    "        # Ensure that this attraction won't be added again from updated_list\n",
    "        updated_list.remove(missing_core_attractions[most_similar_core_index])\n",
    "\n",
    "    # Now, for the remaining attractions (if any)\n",
    "    if x > 0:\n",
    "        other_similarities = []\n",
    "        \n",
    "        for tag in updated_list:\n",
    "            tag_embedding = nlp(tag).vector\n",
    "            similarity = user_input_tag_embedding.dot(tag_embedding) / (np.linalg.norm(user_input_tag_embedding) * np.linalg.norm(tag_embedding))\n",
    "            other_similarities.append(similarity)\n",
    "\n",
    "        sorted_indices = np.argsort(other_similarities)[::-1]  # Descending order\n",
    "        most_similar_tags = [updated_list[i] for i in sorted_indices]\n",
    "        slice_most_similar_tags = most_similar_tags[0:x]\n",
    "        user_input_attractions = user_input_attractions + slice_most_similar_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7324de14",
   "metadata": {},
   "outputs": [],
   "source": [
    "priority_table = pd.DataFrame({\n",
    "    'Attraction': ['Park', 'Tourist Destination', 'Cultural Heritage', 'Scenic Landmarks', 'Nature Attractions',\n",
    "                   'Religious', 'Art',  'Shopping Center', 'Fashion Convenience',\n",
    "                   'Neighborhood Market', 'Gifts & Souvenirs'],\n",
    "    'Opening_Time': ['9:00 AM', '9:00 AM', '11:00 AM', '9:00 AM', '10:00 AM', '11:00 AM', '10:00 AM', \n",
    "                     '10:00 AM', '10:00 AM', '10:00 AM', '10:00 AM'],\n",
    "    'Closing_Time': ['6:00 PM', '6:00 PM', '6:00 PM', '11:00 PM', '6:00 PM', '6:00 PM', '6:00 PM', \n",
    "                     '6:00 PM', '6:00 PM', '6:00 PM', '6:00 PM']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04f7a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the following structure for df_venue_static_att: ['venue_id', 'venue_mod_type']\n",
    "\n",
    "# 1. Get the venue_id for each venue_mod_type from df_venue_static_att\n",
    "venue_ids_per_type = df_venue_static_att.groupby('venue_type')['hash_ven_id'].apply(list).to_dict()\n",
    "\n",
    "# 2. Use the venue_id to filter entries in df_venue_timings\n",
    "hourly_counts = {}\n",
    "for hour in range(24):  # 24 hours\n",
    "    for venue_type, venue_ids in venue_ids_per_type.items():\n",
    "        mask = (df_venue_timings['venue_id'].isin(venue_ids)) & \\\n",
    "               (df_venue_timings['opening_time'] <= hour) & \\\n",
    "               (df_venue_timings['closing_time'] >= hour) & \\\n",
    "               (df_venue_timings['day'] == 6)  # Assuming 6 represents Sunday\n",
    "        count = len(df_venue_timings[mask])\n",
    "        if venue_type not in hourly_counts:\n",
    "            hourly_counts[venue_type] = {}\n",
    "        hourly_counts[venue_type][hour] = count\n",
    "\n",
    "# 3. Determine most common opening and closing times\n",
    "common_times = {}\n",
    "for venue_type, counts in hourly_counts.items():\n",
    "    open_hour = min(counts.keys())\n",
    "    close_hour = max(counts.keys())\n",
    "    common_times[venue_type] = {\n",
    "        'Opening_Time': f'{open_hour}:00 AM' if open_hour < 12 else f'{open_hour-12 if open_hour > 12 else 12}:00 PM',\n",
    "        'Closing_Time': f'{close_hour}:00 AM' if close_hour < 12 else f'{close_hour-12 if close_hour > 12 else 12}:00 PM'\n",
    "    }\n",
    "\n",
    "# 4. Update the priority table\n",
    "for index, row in priority_table.iterrows():\n",
    "    attraction = row['Attraction']\n",
    "    if attraction in common_times:\n",
    "        priority_table.at[index, 'Opening_Time'] = common_times[attraction]['Opening_Time']\n",
    "        priority_table.at[index, 'Closing_Time'] = common_times[attraction]['Closing_Time']\n",
    "\n",
    "#print(priority_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7ee326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suggested Itinerary: {'Tourist Destination': '09:00 AM - 11:00 AM', 'Fashion Convenience': '11:00 AM - 01:00 PM', 'Neighborhood Market': '03:00 PM - 05:00 PM', 'Shopping Center': '05:00 PM - 06:00 PM'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "# Sort attractions based on their opening times\n",
    "priority_table['Opening_Timestamp'] = pd.to_datetime(priority_table['Opening_Time'])\n",
    "sorted_attractions = priority_table.set_index('Attraction').loc[user_input_attractions].sort_values('Opening_Timestamp').index.tolist()\n",
    "\n",
    "# Initialize the itinerary dictionary\n",
    "itinerary = {}\n",
    "\n",
    "# Set the day's starting and ending time\n",
    "start_of_day = pd.Timestamp(f\"{current_date} 9:00 AM\")\n",
    "lunch_start = pd.Timestamp(f\"{current_date} 1:00 PM\")\n",
    "lunch_end = pd.Timestamp(f\"{current_date} 3:00 PM\")\n",
    "dinner_start = pd.Timestamp(f\"{current_date} 7:00 PM\")\n",
    "dinner_end = pd.Timestamp(f\"{current_date} 9:00 PM\")\n",
    "end_of_day = pd.Timestamp(f\"{current_date} 9:00 PM\")\n",
    "current_time = start_of_day\n",
    "\n",
    "for attraction in sorted_attractions:\n",
    "    row = priority_table[priority_table['Attraction'] == attraction].iloc[0]\n",
    "    opening_time = pd.Timestamp(f\"{current_date} {row['Opening_Time']}\")\n",
    "    closing_time = pd.Timestamp(f\"{current_date} {row['Closing_Time']}\")\n",
    "\n",
    "    # Skip if the attraction is already closed or will not open today\n",
    "    if current_time > closing_time or current_time < opening_time:\n",
    "        continue\n",
    "\n",
    "    # If it's lunchtime, jump to after lunch.\n",
    "    if lunch_start <= current_time < lunch_end:\n",
    "        current_time = lunch_end\n",
    "    \n",
    "    # If it's dinnertime, jump to after dinner.\n",
    "    if dinner_start <= current_time < dinner_end:\n",
    "        current_time = dinner_end\n",
    "\n",
    "    # Set the current time to the opening time if it's earlier\n",
    "    if current_time < opening_time:\n",
    "        current_time = opening_time\n",
    "\n",
    "    # Calculate the visit duration (min of 2 hours or available time)\n",
    "    visit_duration = min(2, (closing_time - current_time).seconds / 3600)\n",
    "\n",
    "    # Add to the itinerary if within the day's limit\n",
    "    if current_time + pd.Timedelta(hours=visit_duration) <= end_of_day:\n",
    "        itinerary[attraction] = f\"{current_time.strftime('%I:%M %p')} - {(current_time + pd.Timedelta(hours=visit_duration)).strftime('%I:%M %p')}\"\n",
    "        current_time += pd.Timedelta(hours=visit_duration)  # No buffer added here\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Print the suggested itinerary\n",
    "# print(\"Suggested Itinerary:\", itinerary)\n",
    "itinerary_timing = itinerary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312aef93",
   "metadata": {},
   "source": [
    "## Zone grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47262a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "zone_group = []\n",
    "for group in user_zone_input:\n",
    "    for zone in venue_zone_grouping[group]:\n",
    "        zone_group.append(zone)\n",
    "#zone_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a367d3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "zone_type_dict = {}\n",
    "for venue_type in itinerary_timing:\n",
    "    matched_zones = df_venue_static_att[df_venue_static_att['venue_mod_type'] == venue_type]['zone_id'].unique()\n",
    "    zone_type_dict[venue_type] = list(matched_zones)\n",
    "\n",
    "# Print the resulting dictionary\n",
    "#print(zone_type_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c2f741",
   "metadata": {},
   "source": [
    "## Restaurant Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7912ea88",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_zone_dict = {}\n",
    "for restaurant_type in user_input_restaurants:\n",
    "    # Find the zones where the user's selected restaurant types are located using df_venue_restaurant\n",
    "    matched_zones = df_venue_restaurant[df_venue_restaurant['venue_mod_type'] == restaurant_type]['zone_id'].unique()\n",
    "    restaurant_zone_dict[restaurant_type] = list(matched_zones)\n",
    "\n",
    "# Print the resulting dictionary\n",
    "#print(restaurant_zone_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0305e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants_with_zero_zones = []\n",
    "\n",
    "# Iterate through the restaurant_zone_dict\n",
    "for restaurant_type, zones in restaurant_zone_dict.items():\n",
    "    if len(zones) == 0:\n",
    "        restaurants_with_zero_zones.append(restaurant_type)\n",
    "\n",
    "#print(\"Restaurant types with 0 zones:\", restaurants_with_zero_zones)\n",
    "\n",
    "filled_restaurant_with_zero_zone = {}\n",
    "\n",
    "for res_type in restaurants_with_zero_zones:\n",
    "    restaurant_to_zone_dict_copy = list(set(restaurant_zone_dict[res_type]))\n",
    "    \n",
    "    if len(restaurant_to_zone_dict_copy) <= 0:  # Modified condition\n",
    "        # get all venue id of each zone and push it to restaurant_zone_dict of that type\n",
    "        print('okay')\n",
    "        continue  # Continue to next iteration of the loop\n",
    "    \n",
    "    zone_between_dist = []\n",
    "    for user_zone in zone_group:\n",
    "        for restaurant_zone in restaurant_to_zone_dict_copy:\n",
    "            zone1_polygon = df_manhattan_zone[df_manhattan_zone['LocationID'] == user_zone]['the_geom'].iloc[0]\n",
    "            zone2_polygon = df_manhattan_zone[df_manhattan_zone['LocationID'] == restaurant_zone]['the_geom'].iloc[0]\n",
    "            distance = find_distance_between_zones(zone1_polygon, zone2_polygon)\n",
    "            zone_between_dist.append((user_zone, restaurant_zone, distance)) \n",
    "\n",
    "    sorted_zone_between_dist = sorted(zone_between_dist, key=lambda x: x[2])[:3]\n",
    "    new_zone = [df_venue_restaurant[\n",
    "        (df_venue_restaurant['zone_id'] == item[1]) &\n",
    "        (df_venue_restaurant['venue_mod_type'] == res_type)\n",
    "    ]['original_ven_id'].tolist() for item in sorted_zone_between_dist]\n",
    "    \n",
    "    filled_restaurant_with_zero_zone[res_type] = list(set(item for sublist in new_zone for item in sublist))\n",
    "\n",
    "#print(filled_restaurant_with_zero_zone)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463334eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_venue_dict = {}\n",
    "\n",
    "for restaurant_type, zones in restaurant_zone_dict.items():\n",
    "    # Filter the df_venue_restaurant dataframe for the specific restaurant type and zones\n",
    "    matched_venues = df_venue_restaurant[\n",
    "        (df_venue_restaurant['venue_mod_type'] == restaurant_type) &\n",
    "        (df_venue_restaurant['zone_id'].isin(zones))\n",
    "    ]['original_ven_id'].unique()  # Retrieving the unique venue_ids\n",
    "    \n",
    "    restaurant_venue_dict[restaurant_type] = list(matched_venues)\n",
    "\n",
    "#print(restaurant_venue_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6eccb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_venue_per_type_dict = {}\n",
    "for venue_type in itinerary_timing:\n",
    "    matched_zones = df_venue_static_att[df_venue_static_att['venue_mod_type'] == venue_type]['zone_id']\n",
    "    matching_zones = matched_zones[matched_zones.isin(zone_group)]\n",
    "    result_df = df_venue_static_att[df_venue_static_att['zone_id'].isin(matching_zones)]['original_ven_id']\n",
    "    user_venue_per_type_dict[venue_type] = list(result_df)\n",
    "\n",
    "# Print the resulting dictionary\n",
    "#print(user_venue_per_type_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dcb287",
   "metadata": {},
   "outputs": [],
   "source": [
    "types_with_zero_zones = []\n",
    "\n",
    "# Iterate through the venue_type_dict\n",
    "for venue_type, zones in user_venue_per_type_dict.items():\n",
    "    if len(zones) == 0:\n",
    "        types_with_zero_zones.append(venue_type)\n",
    "\n",
    "#print(\"Venue types with 0 zones:\", types_with_zero_zones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95946cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_distance_between_zones(zone1_polygon, zone2_polygon):\n",
    "    # Find the nearest points between the two polygons\n",
    "    nearest_points_result = nearest_points(wkt.loads(zone1_polygon), wkt.loads(zone2_polygon))\n",
    "\n",
    "    # Calculate the distance between the nearest points\n",
    "    distance = nearest_points_result[0].distance(nearest_points_result[1])\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d0ee8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_type_with_zero_zone = {}\n",
    "\n",
    "for ven_type in types_with_zero_zones:\n",
    "    venue_to_zone_dict_copy = list(set(venue_to_zone_dict[ven_type]))\n",
    "    \n",
    "    if len(venue_to_zone_dict_copy) <= 0: # Modified condition\n",
    "        #get all venue id of each zone and push it to user_venue_per_type_dict of that type\n",
    "        print('okay')\n",
    "        continue  # Continue to next iteration of the loop\n",
    "    \n",
    "    zone_between_dist = []\n",
    "    for user_zone in zone_group:\n",
    "        for venue_zone in venue_to_zone_dict_copy:\n",
    "            zone1_polygon = df_manhattan_zone[df_manhattan_zone['LocationID'] == user_zone]['the_geom'].iloc[0]\n",
    "            zone2_polygon = df_manhattan_zone[df_manhattan_zone['LocationID'] == venue_zone]['the_geom'].iloc[0]\n",
    "            distance = find_distance_between_zones(zone1_polygon, zone2_polygon)\n",
    "            zone_between_dist.append((user_zone, venue_zone, distance)) \n",
    "\n",
    "    sorted_zone_between_dist = sorted(zone_between_dist, key=lambda x: x[2])[:3]\n",
    "    new_zone = [df_venue_static_att[\n",
    "        (df_venue_static_att['zone_id'] == item[1]) &\n",
    "        (df_venue_static_att['venue_mod_type'] == ven_type)\n",
    "    ]['original_ven_id'].tolist() for item in sorted_zone_between_dist]\n",
    "    \n",
    "    filled_type_with_zero_zone[ven_type] = list(set(item for sublist in new_zone for item in sublist))\n",
    "\n",
    "#filled_type_with_zero_zone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdfa136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For venues\n",
    "for key in user_venue_per_type_dict.keys():\n",
    "    # Check if the value of the current key is an empty array\n",
    "    if len(user_venue_per_type_dict[key]) == 0:\n",
    "        # Check if the key exists in the filled dictionary for venues\n",
    "        if key in filled_type_with_zero_zone:\n",
    "            # Replace the value in the main dictionary with the value from the filled dictionary for venues\n",
    "            user_venue_per_type_dict[key] = filled_type_with_zero_zone[key]\n",
    "\n",
    "# For restaurants\n",
    "for key in restaurant_venue_dict.keys():\n",
    "    # Check if the value of the current key is an empty array\n",
    "    if len(restaurant_venue_dict[key]) == 0:\n",
    "        # Check if the key exists in the filled dictionary for restaurants\n",
    "        if key in filled_restaurant_with_zero_zone:\n",
    "            # Replace the value in the main dictionary with the value from the filled dictionary for restaurants\n",
    "            restaurant_venue_dict[key] = filled_restaurant_with_zero_zone[key]\n",
    "\n",
    "#print(user_venue_per_type_dict)\n",
    "#print(restaurant_venue_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e48c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "today_day_num = datetime.now().weekday()  # 0: Monday, 6: Sunday\n",
    "\n",
    "# A function to check if a venue is open today\n",
    "def is_venue_open_today(venue_id, day_num):\n",
    "    venue_today_info = df_venue_timings.loc[(df_venue_timings['venue_id'] == venue_id) & (df_venue_timings['day'] == day_num)]\n",
    "    return not venue_today_info.empty and venue_today_info['opening_time'].iloc[0] != -1 and venue_today_info['closing_time'].iloc[0] != -1\n",
    "\n",
    "# Update user_venue_per_type_dict\n",
    "for attraction_type, venue_ids in user_venue_per_type_dict.items():\n",
    "    user_venue_per_type_dict[attraction_type] = [venue_id for venue_id in venue_ids if is_venue_open_today(venue_id, today_day_num)]\n",
    "\n",
    "#print(user_venue_per_type_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c447c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update restaurant_zone_dict for restaurants open today\n",
    "for restaurant_type, venue_ids in restaurant_venue_dict.items():\n",
    "    restaurant_venue_dict[restaurant_type] = [venue_id for venue_id in venue_ids if is_venue_open_today(venue_id, today_day_num)]\n",
    "\n",
    "#print(restaurant_venue_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244cf4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "manipulated_restaurants = {}\n",
    "\n",
    "# Loop through each restaurant type and check for the specified condition\n",
    "for restaurant_type, restaurant_ids in restaurant_venue_dict.items():\n",
    "    valid_restaurants = []\n",
    "    for restaurant_id in restaurant_ids:\n",
    "        restaurant_hash_id = df_venue_restaurant[df_venue_restaurant['original_ven_id'] == restaurant_id]['hash_ven_id']\n",
    "        restaurant_rating = df_venue_restaurant[df_venue_restaurant['original_ven_id'] == restaurant_id]['rating'].item()\n",
    "        restaurant_hash_id = int(restaurant_hash_id)\n",
    "        \n",
    "        df_venue_merged['venue_id'] = df_venue_merged['venue_id'].astype(int)\n",
    "        specific_restaurant_df = df_venue_merged[df_venue_merged['venue_id'] == restaurant_hash_id]\n",
    "        average_busyness = specific_restaurant_df['busyness'].mean() \n",
    "        \n",
    "        weight_rating = 0.6\n",
    "        weight_busyness = 0.4\n",
    "        composite_score = (weight_rating * restaurant_rating) + (weight_busyness * average_busyness)\n",
    "        \n",
    "        valid_restaurants.append((restaurant_id, restaurant_rating, average_busyness, composite_score))\n",
    "    if valid_restaurants:\n",
    "        manipulated_restaurants[restaurant_type] = valid_restaurants\n",
    "\n",
    "#print(manipulated_restaurants)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fb87b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "manipulated_venues = {}\n",
    "\n",
    "# Loop through each venue type and check for the specified condition\n",
    "for venue_type, venue_ids in user_venue_per_type_dict.items():\n",
    "    valid_venues = []\n",
    "    for venue_id in venue_ids:\n",
    "        venue_hash_id = df_venue_static.loc[df_venue_static['original_ven_id'] == venue_id]['hash_ven_id']\n",
    "        venue_rating = df_venue_static.loc[df_venue_static['original_ven_id'] == venue_id]['rating'].item()\n",
    "        venue_hash_id = int(venue_hash_id)\n",
    "        df_venue_merged['venue_id'] = df_venue_merged['venue_id'].astype(int)\n",
    "        specific_venue_df = df_venue_merged.loc[df_venue_merged['venue_id'] == venue_hash_id]\n",
    "        average_busyness = specific_venue_df['busyness'].mean() \n",
    "        \n",
    "        weight_rating = 0.6\n",
    "        weight_busyness = 0.4\n",
    "        composite_score = (weight_rating * venue_rating) + (weight_busyness * average_busyness)\n",
    "        \n",
    "        \n",
    "        valid_venues.append((venue_id, venue_rating, average_busyness, composite_score))\n",
    "    if valid_venues:\n",
    "        manipulated_venues[venue_type] = valid_venues\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a800e189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'order': 0, 'type': 'FRENCH_RESTAURANT', 'values': [('ven_414a747771393059503062526b6f7759523448483656324a496843', 4.2, 31.303030303030305, 15.041212121212123), ('ven_7379566367365444366462526b6f775a70384d744f38354a496843', 4.5, 27.696969696969695, 13.778787878787877), ('ven_636672745f5152476b4548526b6f7759567052754e786d4a496843', 4.3, 26.325757575757574, 13.110303030303031)], 'type_cat': 'attraction'}, {'order': 1, 'type': 'ITALIAN_RESTAURANT', 'values': [('ven_495a76784756397a636c61526b6f77595659624d79624f4a496843', 4.3, 35.416666666666664, 16.746666666666666), ('ven_6b69536e344b337645554e526b6f775a5a5a764a55525f4a496843', 4.6, 31.59090909090909, 15.396363636363636), ('ven_4168437230416a45783158526b6f7759526c2d6f5339524a496843', 4.5, 29.37121212121212, 14.44848484848485)], 'type_cat': 'attraction'}]\n"
     ]
    }
   ],
   "source": [
    "top_3_restaurants = {}\n",
    "\n",
    "# Loop through each restaurant type and its restaurants\n",
    "for restaurant_type, restaurant_data in manipulated_restaurants.items():\n",
    "    # Sort the restaurants based on the composite score (fourth element in the tuple, index 3)\n",
    "    if len(restaurant_data) > 3:\n",
    "        sorted_restaurants = sorted(restaurant_data, key=lambda x: x[3], reverse=True)\n",
    "    \n",
    "        # Keep only the top 3 restaurants for each restaurant type\n",
    "        top_3_restaurants[restaurant_type] = sorted_restaurants[:3]\n",
    "    else:\n",
    "        top_3_restaurants[restaurant_type] = restaurant_data\n",
    "\n",
    "top_3_restaurants\n",
    "\n",
    "\n",
    "restaurant_keys = list(top_3_restaurants.keys())\n",
    "restaurant_values = list(top_3_restaurants.values())\n",
    "\n",
    "final_restaurants = []\n",
    "for i in range(0, len(restaurant_keys)):\n",
    "    # Check if the index is within the bounds of venue_values\n",
    "    if i < len(restaurant_values):\n",
    "        final_restaurants.append({\n",
    "            'order': i,\n",
    "            'type': restaurant_keys[i],\n",
    "            'values': restaurant_values[i],\n",
    "            'type_cat': 'attraction'\n",
    "        })\n",
    "\n",
    "print(final_restaurants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83e9ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'order': 0, 'type': 'Tourist Destination', 'values': [('ven_673150687856684b77546b526b6f775a644d68505162354a496843', 4.7, 36.36363636363637, 17.365454545454547), ('ven_415a37545a367078557066526b6f775a5a6271774a626e4a496843', 4.6, 33.333333333333336, 16.093333333333334), ('ven_554f66576a594b6a744d5f526b6f775a467258773155664a496843', 3.8, 32.13636363636363, 15.134545454545453)], 'type_cat': 'attraction'}, {'order': 1, 'type': 'Fashion Convenience', 'values': [('ven_63456b77786e474b757563526b6f775a39326e6c5031304a496843', 4.3, 40.78787878787879, 18.895151515151515), ('ven_456c6a3469774243445068526b6f775a3172364d5378424a496843', 4.2, 39.92424242424242, 18.48969696969697), ('ven_51717653514d7730366249526b6f77624e41797a6a39754a496843', 4.2, 38.083333333333336, 17.753333333333334)], 'type_cat': 'attraction'}, {'order': 2, 'type': 'Neighborhood Market', 'values': [('ven_6367354456587959525968526b6f77326a4172446665454a496843', 4.2, 43.20454545454545, 19.80181818181818), ('ven_6f4a3437455573596f624c526b6f77324c414e756756504a496843', 4.1, 42.04545454545455, 19.27818181818182), ('ven_63456b77786e474b757563526b6f775a39326e6c5031304a496843', 4.3, 40.78787878787879, 18.895151515151515)], 'type_cat': 'attraction'}, {'order': 3, 'type': 'Shopping Center', 'values': [('ven_6367354456587959525968526b6f77326a4172446665454a496843', 4.2, 43.20454545454545, 19.80181818181818), ('ven_674c306f645755326f547a526b6f775a68353967465a6c4a496843', 4.2, 40.06060606060606, 18.544242424242427), ('ven_673150687856684b77546b526b6f775a644d68505162354a496843', 4.7, 36.36363636363637, 17.365454545454547)], 'type_cat': 'attraction'}]\n"
     ]
    }
   ],
   "source": [
    "print('|')\n",
    "\n",
    "top_3_venues = {}\n",
    "\n",
    "# Loop through each venue type and its venues\n",
    "for venue_type, venue_data in manipulated_venues.items():\n",
    "    # Sort the venues based on the composite score (fourth element in the tuple, index 3)\n",
    "    if len(venue_data) > 3:\n",
    "        sorted_venues = sorted(venue_data, key=lambda x: x[3], reverse=True)\n",
    "    \n",
    "        # Keep only the top 3 venues for each venue type\n",
    "        top_3_venues[venue_type] = sorted_venues[:3]\n",
    "    else:\n",
    "        top_3_venues[venue_type] = venue_data\n",
    "\n",
    "\n",
    "venue_keys = list(top_3_venues.keys())\n",
    "venue_values = list(top_3_venues.values())\n",
    "\n",
    "final_venues = []\n",
    "for i in range(0, len(venue_keys)):\n",
    "    # Check if the index is within the bounds of venue_values\n",
    "    if i < len(venue_values):\n",
    "        final_venues.append({\n",
    "            'order': i,\n",
    "            'type': venue_keys[i],\n",
    "            'values': venue_values[i],\n",
    "            'type_cat': 'attraction'\n",
    "        })\n",
    "\n",
    "print(final_venues)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
